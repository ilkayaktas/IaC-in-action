A service mesh manages all service-to-service communication within a distributed (potentially microservice-based) software system. It accomplishes this typically via the use of “sidecar” proxies that are deployed alongside each service through which all traffic is transparently routed.
Proxies used within a service mesh are typically “application layer” aware (operating at Layer 7 in the OSI networking stack). This means that traffic routing decisions and the labeling of metrics can draw upon data in HTTP headers or other application layer protocol metadata.
A service mesh provides dynamic service discovery and traffic management, including traffic shadowing (duplicating) for testing, and traffic splitting for canary releasing, incremental rollout, and A/B type experimentation. 
A service mesh also supports the implementation and enforcement of cross cutting requirements, such as security (providing service identity and TLS) and reliability (rate limiting, circuit-breaking).
As a service mesh is on the critical path for every request being handled within the system, it can also provide additional “observability,” such as distributed tracing of a request, frequency of HTTP error codes, global and service-to-service latency.
There are clear benefits provided by the use of a service mesh, but the tradeoffs of added complexity and the requirement of additional runtime resources should be analyzed.
Service mesh technology is rapidly becoming part of the (cloud native) application platform “plumbing.” The interesting innovation within this space is happening in relation to the higher-level abstractions and the human-focused control planes.
Popular service meshes include: Linkerd, Istio, Consul, Kuma, and Maesh. Supporting technologies within this space include: Layer 7-aware proxies, such as Envoy, HAProxy, NGINX, and MOSN; and service mesh orchestration, visualization, and understandability tooling, such as SuperGloo, Kiali, and Dive.